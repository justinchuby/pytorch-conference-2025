{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc5ea2e",
   "metadata": {},
   "source": [
    "# Exporting LLM from transformers to ONNX for running in ONNX Runtime GenAI\n",
    "\n",
    "In this tutorial, you will learn how to export a model from transformers to ONNX, optimize it for ONNX Runtime, and run it with ONNX Runtime GenAI. The model exported will be in float32. For model quantization and better performance, refer to Olive and many other quantization tools available for ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c052d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install required packages. To use torch.onnx export, you need the latest version of onnxscript\n",
    "%pip install transformers==4.55 torch==2.9.0\n",
    "%pip install --upgrade onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1668c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/onnx/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a71270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we will use Gemma-3 model\n",
    "# MODEL_ID = \"google/gemma-3-270m-it\"\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "# MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "# MODEL_ID = \"google/gemma-3-27b-it\"\n",
    "\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada633c",
   "metadata": {},
   "source": [
    "### Make transformers model exportable\n",
    "\n",
    "There are some tweaks we need to do to the model so that it is torch.export friendly. We do that by registering a simplified version of the attention implementation to transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01266f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(\n",
    "        batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "    )\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def sdpa_attention_forward_with_check(\n",
    "    module: torch.nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: torch.Tensor | None,\n",
    "    dropout: float = 0.0,\n",
    "    scaling: float | None = None,\n",
    "    is_causal: bool | None = None,\n",
    "    **kwargs,\n",
    ") -> tuple[torch.Tensor, None]:\n",
    "    if hasattr(module, \"num_key_value_groups\"):\n",
    "        key = repeat_kv(key, module.num_key_value_groups)\n",
    "        value = repeat_kv(value, module.num_key_value_groups)\n",
    "    if attention_mask is not None and attention_mask.ndim == 4:\n",
    "        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
    "        torch._check(\n",
    "            attention_mask.shape[-1] == key.shape[-2],\n",
    "            lambda: \"attention_mask.shape[-1] == key.shape[-2] should be true\",\n",
    "        )\n",
    "\n",
    "    if is_causal is None:\n",
    "        is_causal = (\n",
    "            query.shape[2] > 1\n",
    "            and attention_mask is None\n",
    "            and getattr(module, \"is_causal\", True)\n",
    "        )\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=dropout,\n",
    "        scale=scaling,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, None\n",
    "\n",
    "\n",
    "# Patch the attention functions to use the custom SDPA without vmap\n",
    "# This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n",
    "ALL_MASK_ATTENTION_FUNCTIONS.register(\n",
    "    \"sdpa_without_vmap\",\n",
    "    transformers.integrations.executorch.sdpa_mask_without_vmap,\n",
    ")\n",
    "ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_attention_forward_with_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98baf922",
   "metadata": {},
   "source": [
    "### Prepare the model and example inputs for tracing\n",
    "\n",
    "Load the model from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33761c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_model(model_id: str):\n",
    "    \"\"\"Load a Hugging Face model and its config.\"\"\"\n",
    "    # We use our custom attention implementation here\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_id, attn_implementation=\"sdpa_without_vmap\"\n",
    "    )\n",
    "    config.use_cache = True\n",
    "    # Use the correct AutoModel class for your model architecture\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, config=config)\n",
    "\n",
    "    return model, config\n",
    "\n",
    "\n",
    "model, config = get_hf_model(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164875cc",
   "metadata": {},
   "source": [
    "Then, create example inputs and specify all the input and output names. Be sure to always provide a example inputs with dimension size>=2 when the dimension is specified to be dynamic. This is due to the [0/1 specialization problem](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ&tab=t.0#heading=h.ez923tomjvyk) in PyTorch.\n",
    "\n",
    "Refer to the torch.export documentation for examples on how you can provide dynamic shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3302b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dynamic_cache(\n",
    "    past_key_values: list[tuple[torch.Tensor, torch.Tensor]],\n",
    ") -> transformers.cache_utils.DynamicCache:\n",
    "    \"\"\"Create a DynamicCache from past_key_values.\"\"\"\n",
    "    cache = transformers.cache_utils.DynamicCache()\n",
    "    for layer_idx in range(len(past_key_values)):\n",
    "        key_states, value_states = past_key_values[layer_idx]\n",
    "        cache.update(key_states, value_states, layer_idx)\n",
    "    return cache\n",
    "\n",
    "\n",
    "def create_text_gen_example_inputs(\n",
    "    config, batch_size: int = 2, seq_len: int = 3, past_seq_len: int = 2\n",
    "):\n",
    "    \"\"\"Create example inputs and dynamic axes for ONNX export.\"\"\"\n",
    "    config = config.get_text_config()\n",
    "    num_hidden_layers = config.num_hidden_layers\n",
    "    # batch = \"batch\"\n",
    "    # sequence_len = \"sequence_len\"\n",
    "    # past_sequence_len = \"past_sequence_len\"\n",
    "    batch = torch.export.Dim(\"batch\")\n",
    "    sequence_len = torch.export.Dim(\"sequence_len\")\n",
    "    # past_sequence_len = torch.export.Dim(\"past_sequence_len\")\n",
    "\n",
    "    dynamic_shapes = {\n",
    "        \"input_ids\": {0: batch, 1: sequence_len},\n",
    "        \"attention_mask\": {\n",
    "            0: batch,\n",
    "            1: \"past_sequence_len+sequence_len\",\n",
    "        },\n",
    "        \"position_ids\": {\n",
    "            0: batch,\n",
    "            1: sequence_len,\n",
    "        },\n",
    "        \"past_key_values\": [\n",
    "            [{0: batch, 2: \"past_sequence_len\"} for _ in range(num_hidden_layers)],\n",
    "            [{0: batch, 2: \"past_sequence_len\"} for _ in range(num_hidden_layers)],\n",
    "        ],\n",
    "    }\n",
    "    input_names = [\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"position_ids\",\n",
    "        *[f\"past_key_values.{i}.key\" for i in range(num_hidden_layers)],\n",
    "        *[f\"past_key_values.{i}.value\" for i in range(num_hidden_layers)],\n",
    "    ]\n",
    "    output_names = [\n",
    "        \"logits\",\n",
    "        *[f\"present.{i}.key\" for i in range(num_hidden_layers)],\n",
    "        *[f\"present.{i}.value\" for i in range(num_hidden_layers)],\n",
    "    ]\n",
    "\n",
    "    num_key_value_heads = config.num_key_value_heads\n",
    "    head_dim = config.head_dim\n",
    "\n",
    "    example_inputs = dict(\n",
    "        input_ids=torch.randint(0, 2, (batch_size, seq_len), dtype=torch.int64),\n",
    "        attention_mask=torch.ones(\n",
    "            (batch_size, past_seq_len + seq_len),\n",
    "            dtype=torch.int64,\n",
    "        ),\n",
    "        position_ids=torch.arange(\n",
    "            past_seq_len,\n",
    "            past_seq_len + seq_len,\n",
    "            dtype=torch.int64,\n",
    "        ).expand((batch_size, -1)),\n",
    "        past_key_values=make_dynamic_cache(\n",
    "            [\n",
    "                (\n",
    "                    torch.randn(\n",
    "                        batch_size,\n",
    "                        num_key_value_heads,\n",
    "                        seq_len,\n",
    "                        head_dim,\n",
    "                    ),\n",
    "                    torch.randn(\n",
    "                        batch_size,\n",
    "                        num_key_value_heads,\n",
    "                        seq_len,\n",
    "                        head_dim,\n",
    "                    ),\n",
    "                )\n",
    "                for _ in range(num_hidden_layers)\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return example_inputs, dynamic_shapes, input_names, output_names\n",
    "\n",
    "\n",
    "# Obtain example inputs and dynamic axes\n",
    "example_kwargs, dynamic_shapes, input_names, output_names = (\n",
    "    create_text_gen_example_inputs(config)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbdb35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_names: ['input_ids', 'attention_mask', 'position_ids', 'past_key_values.0.key', 'past_key_values.1.key', 'past_key_values.2.key', 'past_key_values.3.key', 'past_key_values.4.key', 'past_key_values.5.key', 'past_key_values.6.key', 'past_key_values.7.key', 'past_key_values.8.key', 'past_key_values.9.key', 'past_key_values.10.key', 'past_key_values.11.key', 'past_key_values.12.key', 'past_key_values.13.key', 'past_key_values.14.key', 'past_key_values.15.key', 'past_key_values.16.key', 'past_key_values.17.key', 'past_key_values.18.key', 'past_key_values.19.key', 'past_key_values.20.key', 'past_key_values.21.key', 'past_key_values.22.key', 'past_key_values.23.key', 'past_key_values.24.key', 'past_key_values.25.key', 'past_key_values.0.value', 'past_key_values.1.value', 'past_key_values.2.value', 'past_key_values.3.value', 'past_key_values.4.value', 'past_key_values.5.value', 'past_key_values.6.value', 'past_key_values.7.value', 'past_key_values.8.value', 'past_key_values.9.value', 'past_key_values.10.value', 'past_key_values.11.value', 'past_key_values.12.value', 'past_key_values.13.value', 'past_key_values.14.value', 'past_key_values.15.value', 'past_key_values.16.value', 'past_key_values.17.value', 'past_key_values.18.value', 'past_key_values.19.value', 'past_key_values.20.value', 'past_key_values.21.value', 'past_key_values.22.value', 'past_key_values.23.value', 'past_key_values.24.value', 'past_key_values.25.value']\n",
      "output_names: ['logits', 'present.0.key', 'present.1.key', 'present.2.key', 'present.3.key', 'present.4.key', 'present.5.key', 'present.6.key', 'present.7.key', 'present.8.key', 'present.9.key', 'present.10.key', 'present.11.key', 'present.12.key', 'present.13.key', 'present.14.key', 'present.15.key', 'present.16.key', 'present.17.key', 'present.18.key', 'present.19.key', 'present.20.key', 'present.21.key', 'present.22.key', 'present.23.key', 'present.24.key', 'present.25.key', 'present.0.value', 'present.1.value', 'present.2.value', 'present.3.value', 'present.4.value', 'present.5.value', 'present.6.value', 'present.7.value', 'present.8.value', 'present.9.value', 'present.10.value', 'present.11.value', 'present.12.value', 'present.13.value', 'present.14.value', 'present.15.value', 'present.16.value', 'present.17.value', 'present.18.value', 'present.19.value', 'present.20.value', 'present.21.value', 'present.22.value', 'present.23.value', 'present.24.value', 'present.25.value']\n"
     ]
    }
   ],
   "source": [
    "print(\"input_names:\", input_names)\n",
    "print(\"output_names:\", output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1206001d",
   "metadata": {},
   "source": [
    "Now, define a wrapper for the transformers model so that it takes `torch.Tensor`s as inputs and returns `torch.Tensor`s as outputs. By keeping the model signature simple, we make the job of understanding the model IO easier for `torch.export` especially when there is dynamic shapes involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45fc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationModelWrapper(torch.nn.Module):\n",
    "    \"\"\"A wrapper around a Hugging Face model to adjust the forward method for ONNX export.\"\"\"\n",
    "\n",
    "    def __init__(self, model: transformers.PreTrainedModel):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        position_ids,\n",
    "        past_key_values,\n",
    "    ):\n",
    "        hf_output = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        return hf_output.logits, hf_output.past_key_values\n",
    "\n",
    "\n",
    "# Wrap the model to adjust the forward method for ONNX export\n",
    "model = TextGenerationModelWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5eabd",
   "metadata": {},
   "source": [
    "### Export the model\n",
    "\n",
    "With everything ready, we can now call `torch.onnx.export` to export the model. Internally, `torch.onnx.export` calls `torch.export` to obtain the model graph, then translates the model to ONNX using ONNX Script and `onnx-ir`.\n",
    "\n",
    "Set `opset_version` to 23 if you want to get the ONNX standard Attention and RotaryEmbedding ops. Set it to 20 (as of Oct 2025) if you want to do fusion for ONNX Runtime. As we improve the fusion logic, we expect opset 23 and higher to be the generally supported and recommended opsets very soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4e1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1017 18:57:12.658000 41808 site-packages/torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `TextGenerationModelWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `TextGenerationModelWrapper([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/onnx/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dynamic_shapes.py:264: UserWarning: # The axis name: batch will not be used, since it shares the same shape constraints with another axis: batch.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/onnx/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dynamic_shapes.py:264: UserWarning: # The axis name: sequence_len will not be used, since it shares the same shape constraints with another axis: sequence_len.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/onnx/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_dynamic_shapes.py:264: UserWarning: # The axis name: past_sequence_len will not be used, since it shares the same shape constraints with another axis: past_sequence_len.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 474 of general pattern rewrite rules.\n",
      "✅ Export successful\n"
     ]
    }
   ],
   "source": [
    "onnx_program = torch.onnx.export(\n",
    "    model,\n",
    "    (),\n",
    "    kwargs=example_kwargs,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_shapes=dynamic_shapes,\n",
    "    opset_version=20,  # Set to 20 for ORT fusion rules\n",
    "    dynamo=True,\n",
    "    # report=True,  # Uncomment to get a report of the export\n",
    ")\n",
    "\n",
    "print(\"✅ Export successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7f344",
   "metadata": {},
   "source": [
    "Now, we can call ONNX Script optimizer to optimize for ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c561dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize the model with ONNX Runtime custom ops...\n",
      "Applied 1 of general pattern rewrite rules.\n",
      "Applied fusions: {'erf_gelu': 0, 'rms_normalization': 157, 'skip_layer_normalization': 0, 'skip_rms_normalization': 52, 'rotary_embedding': 52, 'cos_sin_cache': 52, 'partial_rotary_embedding': 0, 'sdpa': 19, 'gqa': 19, 'packed_qkv_for_gqa': 0, 'mha1': 0, 'mha2': 0, 'mha_scale': 0, 'mha_bias': 0, 'attention': 0, 'gelu': 0, 'bias_gelu': 0}\n"
     ]
    }
   ],
   "source": [
    "from onnxscript.rewriter.ort_fusions import optimize_for_ort\n",
    "\n",
    "print(\"Optimize the model with ONNX Runtime custom ops...\")\n",
    "\n",
    "# Get the onnx_ir Model with onnx_program.model. The fusion is done inplace.\n",
    "_, count = optimize_for_ort(onnx_program.model)\n",
    "print(f\"Applied fusions: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539bb5f",
   "metadata": {},
   "source": [
    "Save the model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d57ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "model_dir = pathlib.Path(f\"models/{MODEL_NAME}_ort\")\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a80089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Model saved to models/gemma-3-1b-it_ort/gemma-3-1b-it.onnx\n"
     ]
    }
   ],
   "source": [
    "# Use the ONNXProgram.save method to save the model. Specifying external_data=True\n",
    "# will save the model weights in external files, which is required for models > 2GB\n",
    "path = model_dir / f\"{MODEL_NAME}.onnx\"\n",
    "onnx_program.save(path, external_data=True)\n",
    "\n",
    "print(f\"🧠 Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a2df2",
   "metadata": {},
   "source": [
    "## Run with ONNX Runtime GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ff222",
   "metadata": {},
   "source": [
    "First load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b36533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "model_path = f\"models/{model_name}_ort\"\n",
    "\n",
    "# Create tokenizer files if they don't exist\n",
    "if not (model_dir / \"tokenizer_config.json\").exists():\n",
    "    print(\"Downloading tokenizer...\")\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    AutoTokenizer.from_pretrained(MODEL_ID).save_pretrained(model_dir)\n",
    "\n",
    "model = og.Model(str(model_dir))\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "# Set the max length to something sensible by default,\n",
    "# since otherwise it will be set to the entire context length\n",
    "search_options = {}\n",
    "search_options[\"max_length\"] = 2048\n",
    "search_options[\"batch_size\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2eb84b",
   "metadata": {},
   "source": [
    "Now generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de211d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The sky is blue due to a phenomenon called **Rayleigh scattering**. Here's a breakdown of how it works:\n",
      "\n",
      "* **Sunlight is made of all colors:** White sunlight is actually a mixture of all the colors of the rainbow – red, orange, yellow, green, blue, indigo, and violet.\n",
      "\n",
      "* **Entering the atmosphere:** When sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).\n",
      "\n",
      "* **Scattering of light:** This collision causes the light to scatter in different directions.▁▁Rayleigh scattering is a type of scattering that's more effective at shorter wavelengths of light.\n",
      "\n",
      "* **Blue light is scattered more:** Blue and violet light have shorter wavelengths than other colors like red and orange.▁▁Because of this, they are scattered much more strongly by the air molecules.▁▁It's like throwing a small pebble (blue light) at a bumpy surface – it bounces in all directions.\n",
      "\n",
      "* **Why we see blue, not violet:** While violet light is scattered even *more* than blue, there are a couple of reasons why we see a blue sky instead of a violet one:\n",
      "▁▁▁▁* **Sunlight emits less violet:** The sun doesn't emit as much violet light as blue light.\n",
      "▁▁▁▁* **Our eyes are less sensitive to violet:** Human eyes are more sensitive to blue light than violet light.\n",
      "\n",
      "\n",
      "**In short, the atmosphere scatters blue light more than other colors, making the sky appear blue to our eyes.**\n",
      "\n",
      "**Think of it like this:** Imagine throwing a handful of marbles (blue light) and a handful of tennis balls (red light) at a bumpy surface. The marbles are more likely to bounce off in all directions, while the tennis balls are more likely to travel in a straight line.\n",
      "\n",
      "\n",
      "Do you want to learn more about:\n",
      "\n",
      "*▁▁▁**Why sunsets are red/orange?**\n",
      "*▁▁▁**How the atmosphere affects the color of clouds?**\n"
     ]
    }
   ],
   "source": [
    "text = \"Why is the sky blue?\"\n",
    "\n",
    "# Generate the prompt by applying the chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages=f\"\"\"[{{\"role\": \"user\", \"content\": \"{text}\"}}]\"\"\",\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(**search_options)\n",
    "generator = og.Generator(model, params)\n",
    "\n",
    "print(\"Output: \", end=\"\", flush=True)\n",
    "\n",
    "try:\n",
    "    generator.append_tokens(input_tokens)\n",
    "    while not generator.is_done():\n",
    "        generator.generate_next_token()\n",
    "\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        print(tokenizer_stream.decode(new_token), end=\"\", flush=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"  --control+c pressed, aborting generation--\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e796b9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You have successfully completed the journey of exporting a Hugging Face transformers model to ONNX and running it with ONNX Runtime GenAI. Here's what you accomplished:\n",
    "\n",
    "### Key Steps Completed:\n",
    "1. **Model Preparation**: Modified the Gemma-3 model to be torch.export friendly by implementing custom attention functions that avoid non-exportable operations like `vmap`\n",
    "2. **Export Setup**: Created proper example inputs with dynamic shapes and defined input/output names for the ONNX export process\n",
    "3. **Model Wrapping**: Wrapped the transformers model to have a clean tensor-based interface suitable for ONNX export\n",
    "4. **ONNX Export**: Successfully exported the model using `torch.onnx.export` with opset version 20 for optimal ONNX Runtime compatibility\n",
    "5. **Optimization**: Applied ONNX Runtime-specific fusion optimizations to improve performance\n",
    "6. **Integration**: Loaded the exported model in ONNX Runtime GenAI and demonstrated text generation capabilities\n",
    "\n",
    "### Next Steps:\n",
    "- For even better performance, consider quantizing the model using tools like Olive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb5d3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4536e4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
